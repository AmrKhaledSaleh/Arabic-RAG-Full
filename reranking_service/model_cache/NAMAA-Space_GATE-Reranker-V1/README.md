---
license: apache-2.0
language:
- ar
pipeline_tag: text-classification
library_name: transformers
base_model:
- Omartificial-Intelligence-Space/GATE-AraBert-v1
tags:
- reranking
- sentence-transformers
---

# GATE-Reranker-V1 ğŸš€âœ¨

**NAMAA-space** releases **GATE-Reranker-V1**, a high-performance model fine-tuned to elevate Arabic document retrieval and ranking to new heights! ğŸ“šğŸ‡¸ğŸ‡¦

This model is designed to **improve search relevance** of **arabic** documents by accurately ranking documents based on their contextual fit for a given query.

## Key Features ğŸ”‘

- **Optimized for Arabic**: Built on the highly performant [Omartificial-Intelligence-Space/GATE-AraBert-v1](https://huggingface.co/Omartificial-Intelligence-Space/GATE-AraBert-v1) with exclusivly rich Arabic data.
- **Advanced Document Ranking**: Ranks results with precision, perfect for search engines, recommendation systems, and question-answering applications.
- **State-of-the-Art Performance**: Achieves excellent performance compared to famous rerankers(See [Evaluation](https://huggingface.co/NAMAA-Space/GATE-Reranker-V1#evaluation)), ensuring reliable relevance and precision.

## Example Use Cases ğŸ’¼

- **Retrieval Augmented Generation**: Improve search result relevance for Arabic content.
- **Content Recommendation**: Deliver top-tier Arabic content suggestions.
- **Question Answering**: Boost answer retrieval quality in Arabic-focused systems.

## Usage 

# Within sentence-transformers
The usage becomes easier when you have [SentenceTransformers](https://www.sbert.net/) installed. Then, you can use the pre-trained models like this:

```python
from sentence_transformers import CrossEncoder
model = CrossEncoder('NAMAA-Space/GATE-Reranker-V1', max_length=512)

Query = 'ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ©ØŸ'
Paragraph1 = 'Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ ÙŠØ³Ø§Ø¹Ø¯ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ© ÙˆØªØ´Ø®ÙŠØµ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶'
Paragraph2 = 'Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠØ³ØªØ®Ø¯Ù… ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© ÙÙŠ Ø§Ù„ØµÙ†Ø§Ø¹Ø§Øª'

scores = model.predict([(Query, Paragraph1), (Query, Paragraph2)])
```

## Evaluation

We evaluate our model on two different datasets using the metrics **MAP**, **MRR** and **NDCG@10**: 

The purpose of this evaluation is to highlight the performance of our model with regards to: Relevant/Irrelevant labels and positive/multiple negatives documents:

Dataset 1: [NAMAA-Space/Ar-Reranking-Eval](https://huggingface.co/datasets/NAMAA-Space/Ar-Reranking-Eval)

![Plot](https://huggingface.co/NAMAA-Space/GATE-Reranker-V1/resolve/main/Dataset1_Evaluation.jpg)

Dataset 2: [NAMAA-Space/Arabic-Reranking-Triplet-5-Eval](https://huggingface.co/datasets/NAMAA-Space/Arabic-Reranking-Triplet-5-Eval)

![Plot](https://huggingface.co/NAMAA-Space/GATE-Reranker-V1/resolve/main/Dataset2_Evaluation.jpg)

As seen, The model performs extremly well in comparison to other famous rerankers. 

WIP: More comparisons and evaluation on arabic datasets.